{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MitaliKambli/J025_NLP/blob/master/Classwork/Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGUTDyb_TF12",
        "colab_type": "code",
        "outputId": "9a2861d3-af01-4cbd-c6ad-b71f03bfd327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import pandas as pd\n",
        "import urllib, json\n",
        "df=pd.read_json(\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Office_Products_5.json.gz\",lines=True)\n",
        "df=df.head(100)\n",
        "print(df)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "        reviewerID        asin  ... unixReviewTime   reviewTime\n",
            "0   A32T2H8150OJLU  B00000JBLH  ...     1094169600   09 3, 2004\n",
            "1   A3MAFS04ZABRGO  B00000JBLH  ...     1197676800  12 15, 2007\n",
            "2   A1F1A0QQP2XVH5  B00000JBLH  ...     1293840000   01 1, 2011\n",
            "3    A49R5DBXXQDE5  B00000JBLH  ...     1145404800  04 19, 2006\n",
            "4   A2XRMQA6PJ5ZJ8  B00000JBLH  ...     1375574400   08 4, 2013\n",
            "..             ...         ...  ...            ...          ...\n",
            "95  A304GUEPCPYM3Z  B00000JZKB  ...     1392163200  02 12, 2014\n",
            "96  A10JPZAYDGFHEV  B00000JZKB  ...     1387497600  12 20, 2013\n",
            "97   A8LEFDO3AKIJP  B00000JZKB  ...     1365984000  04 15, 2013\n",
            "98   AIKGBCYBXVH8I  B00000JZKB  ...     1303084800  04 18, 2011\n",
            "99  A2L9XBP14SK6IU  B00000JZKB  ...     1393718400   03 2, 2014\n",
            "\n",
            "[100 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIpnOVq2f94R",
        "colab_type": "code",
        "outputId": "ad3e79c8-c270-41fb-fb79-77feff728bf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize    \n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "def clean_list(sentence): \n",
        "  tokenizer = RegexpTokenizer(r'\\w+') \n",
        "  word_tokens=tokenizer.tokenize(sentence)\n",
        "  filtered_sentence = [] \n",
        "  for w in word_tokens: \n",
        "      if w not in stop_words:\n",
        "        if w.isalpha():\n",
        "          filtered_sentence.append(w) \n",
        "  return filtered_sentence\n",
        "\n",
        "cleanl= df['reviewText'].apply(clean_list)\n",
        "cleanl=cleanl.to_list()\n",
        "import itertools\n",
        "clean = list(itertools.chain(*cleanl))\n",
        "clean=list(set(clean))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mARq9czEnH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "itertools.product(clean,clean)\n",
        "combo=list(itertools.product(clean,clean))\n",
        "#combo[:300]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCUFSfB65UyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "f4ad4853-e25e-4fbe-cfca-9b718132534d"
      },
      "source": [
        "import nltk\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim import models\n",
        "\n",
        "# build vocabulary and train model\n",
        "model = Word2Vec(cleanl,size=100, window=5, min_count=1, workers=4)\n",
        "#print(model.wv.vocab)\n",
        "for i in range (20):\n",
        "  print(combo[i][0],combo[i][1],' : ',model.similarity(combo[i][0],combo[i][1]))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "find find  :  1.0\n",
            "find duplex  :  0.17514788\n",
            "find background  :  0.023366826\n",
            "find pixel  :  -0.14988504\n",
            "find better  :  0.3063064\n",
            "find ultimate  :  0.256978\n",
            "find commercial  :  0.060971156\n",
            "find input  :  -0.024550654\n",
            "find rise  :  0.077576265\n",
            "find buyers  :  0.26899073\n",
            "find laptop  :  0.12393367\n",
            "find star  :  0.15508918\n",
            "find calculating  :  -0.0017201484\n",
            "find traditional  :  0.15092243\n",
            "find Oh  :  -0.072092354\n",
            "find deal  :  -0.113199815\n",
            "find entering  :  0.053718593\n",
            "find required  :  -0.073767476\n",
            "find ripped  :  0.082260855\n",
            "find The  :  0.36563313\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}